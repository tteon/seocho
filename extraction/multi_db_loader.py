
import os
import ast
import pandas as pd
from neo4j import GraphDatabase

# Configuration per user snippet
DB_MAPPING = {
    "BASELINE": "kgnormal",
    "FIBO": "kgfibo"
}

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://neo4j:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password")

def parse_record(record_str):
    try:
        if isinstance(record_str, str):
            # Safe eval for python literals
            return ast.literal_eval(record_str)
        return record_str
    except (ValueError, SyntaxError):
        return {}

def ingest_to_specific_db(driver, df, mode, target_db):
    print(f"\nğŸš€ [Mode: {mode}] -> [DB: {target_db}] ì ì¬ ì‹œì‘ (ì´ {len(df)}ê±´)")
    
    # ì¿¼ë¦¬: ë…¸ë“œ ìƒì„± (Query: Create Nodes)
    node_query = """
    UNWIND $batch AS row
    MERGE (e:Entity {name: row.text})
    ON CREATE SET e.type = row.type, e.source_mode = $mode
    """
    
    try:
        # Use session with specific database
        with driver.session(database=target_db) as session:
            
            # --- [Step 1] Load Nodes ---
            batch_nodes = []
            for _, row in df.iterrows():
                # parsing logic assumes input df has 'extracted_entities' column
                # which is a string repr of dict: {'extracted_entities': [...]}
                parsed_ent = parse_record(row.get('extracted_entities', '{}'))
                entities = parsed_ent.get('extracted_entities', [])
                
                # Adapting to potentially different structure if needed, 
                # but following user snippet strictly for now.
                # User's snippet assumes entities have 'text' and 'type' keys presumably.
                # Let's verify structure in a real run, but here we implement the logic.
                batch_nodes.extend(entities)
                
                if len(batch_nodes) >= 1000:
                    session.run(node_query, batch=batch_nodes, mode=mode)
                    batch_nodes = []
            
            if batch_nodes:
                session.run(node_query, batch=batch_nodes, mode=mode)
            print(f"   âœ… ë…¸ë“œ ìƒì„± ì™„ë£Œ (Nodes Created @{target_db})")

            # --- [Step 2] Load Relationships ---
            rels_by_type = {}
            for _, row in df.iterrows():
                parsed_rel = parse_record(row.get('linked_relationships', '{}'))
                relationships = parsed_rel.get('entity_relationships', [])
                
                for rel in relationships:
                    r_type = rel.get('relation_type', 'RELATED').strip().upper().replace(" ", "_")
                    if not r_type: continue
                    
                    if r_type not in rels_by_type:
                        rels_by_type[r_type] = []
                    
                    rels_by_type[r_type].append({
                        "source": rel.get('source_entity'),
                        "target": rel.get('target_entity')
                    })

            count_rels = 0
            for r_type, batch_data in rels_by_type.items():
                rel_query = f"""
                UNWIND $batch AS row
                MATCH (source:Entity {{name: row.source}})
                MATCH (target:Entity {{name: row.target}})
                MERGE (source)-[:{r_type}]->(target)
                """
                
                batch_size = 1000
                for i in range(0, len(batch_data), batch_size):
                    chunk = batch_data[i:i + batch_size]
                    session.run(rel_query, batch=chunk)
                    count_rels += len(chunk)
                    
            print(f"   ğŸ”— ê´€ê³„ ì—°ê²° ì™„ë£Œ (Rels Linked @{target_db}, {count_rels}ê±´)")
            
    except Exception as e:
        print(f"âŒ [Error] '{target_db}' ë°ì´í„°ë² ì´ìŠ¤ ì ‘ì† ì‹¤íŒ¨: {e}")
        print("   (Tip: Enterprise ë²„ì „ì´ ì•„ë‹ˆê±°ë‚˜, DBê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

def run_multidb_ingest(full_df):
    # Driver connected once
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    
    try:
        driver.verify_connectivity()
    except Exception as e:
        print(f"âŒ Neo4j ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # Check for 'experiment_mode' column
    if 'experiment_mode' not in full_df.columns:
        print("âš ï¸ 'experiment_mode' column missing. Defaulting to 'BASELINE'.")
        full_df['experiment_mode'] = 'BASELINE'

    modes = full_df['experiment_mode'].unique()
    print(f"ğŸ” ê°ì§€ëœ ì‹¤í—˜ ëª¨ë“œ (Detected Modes): {modes}")
    
    for mode in modes:
        if mode not in DB_MAPPING:
            print(f"âš ï¸ ê²½ê³ : ëª¨ë“œ '{mode}'ì— ë§¤í•‘ëœ DBê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        target_db_name = DB_MAPPING[mode]
        subset_df = full_df[full_df['experiment_mode'] == mode]
        
        # Reuse driver
        ingest_to_specific_db(driver, subset_df, mode, target_db_name)

    driver.close()
    print("\nğŸ‰ ëª¨ë“  ë°ì´í„° ì ì¬ ì™„ë£Œ (All Data Loaded)!")

if __name__ == "__main__":
    # Example mock run
    print("Running Multi-DB Ingest Demo...")
    # Mock DataFrame matching user structure expectation
    # 'extracted_entities': stringified dict with list of entities
    # 'linked_relationships': stringified dict with list of rels
    
    mock_data = [
        {
            "experiment_mode": "BASELINE",
            "extracted_entities": str({"extracted_entities": [{"text": "Apple", "type": "ORG"}, {"text": "iPhone", "type": "PRODUCT"}]}),
            "linked_relationships": str({"entity_relationships": [{"source_entity": "Apple", "target_entity": "iPhone", "relation_type": "MAKES"}]})
        },
        {
            "experiment_mode": "FIBO",
            "extracted_entities": str({"extracted_entities": [{"text": "Interest Rate", "type": "INDICATOR"}, {"text": "Fed", "type": "ORG"}]}),
            "linked_relationships": str({"entity_relationships": [{"source_entity": "Fed", "target_entity": "Interest Rate", "relation_type": "SETS"}]})
        }
    ]
    df = pd.DataFrame(mock_data)
    
    # Ensure DBs exist first (calling the other script logic ideally, or assuming user ran manage_databases.py)
    # run_multidb_ingest(df)
    print("Call run_multidb_ingest(df) to execute.")
