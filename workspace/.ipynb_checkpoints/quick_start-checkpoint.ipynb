{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Quick Start: Opik + OpenAI + DotEnv\n",
                "\n",
                "This notebook demonstrates how to:\n",
                "1. Load environment variables using `python-dotenv`.\n",
                "2. Initialize OpenAI client.\n",
                "3. Integrate Opik for tracing.\n",
                "4. Use Opik for Prompt Management."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Environment Setup\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "print(\"Environment variables loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Opik Setup & Tracing\n",
                "import opik\n",
                "from opik.integrations.openai import track_openai\n",
                "from openai import OpenAI\n",
                "\n",
                "# Ensure Opik points to the host instance (using the URL override from docker-compose)\n",
                "# docker-compose.yml sets OPIK_URL_OVERRIDE=http://host.docker.internal:5173/api\n",
                "# We can verify this:\n",
                "print(f\"Opik URL: {os.getenv('OPIK_URL_OVERRIDE')}\")\n",
                "\n",
                "# Initialize OpenAI client\n",
                "client = OpenAI()\n",
                "\n",
                "# Wrap the client with Opik tracker\n",
                "openai_client = track_openai(client)\n",
                "\n",
                "print(\"Opik configured and OpenAI client wrapped.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Run a Trace\n",
                "try:\n",
                "    completion = openai_client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "            {\"role\": \"user\", \"content\": \"Write a haiku about observability.\"}\n",
                "        ]\n",
                "    )\n",
                "    print(\"\\nResponse:\")\n",
                "    print(completion.choices[0].message.content)\n",
                "    print(\"\\nCheck your Opik dashboard for the trace!\")\n",
                "except Exception as e:\n",
                "    print(f\"Error making request: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Prompt Management\n",
                "try:\n",
                "    # Create or retrieve a prompt\n",
                "    prompt_name = \"observability-story-1\"\n",
                "    prompt_template = \"Write a short story about {{topic}} using {{adjective}} language.\"\n",
                "    \n",
                "    prompt = opik.Prompt(\n",
                "        name=prompt_name,\n",
                "        prompt=prompt_template\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nPrompt '{prompt.name}' created/retrieved.\")\n",
                "    print(f\"Template: {prompt.prompt}\")\n",
                "    \n",
                "    # Format the prompt\n",
                "    formatted_prompt = prompt.format(topic=\"AI agents\", adjective=\"flowery\")\n",
                "    print(f\"\\nFormatted Prompt:\\n{formatted_prompt}\")\n",
                "    \n",
                "    # Use the formatted prompt in a call (traced)\n",
                "    completion = openai_client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
                "    )\n",
                "    \n",
                "    print(\"\\nStory Response:\")\n",
                "    print(completion.choices[0].message.content)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error in prompt management: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}